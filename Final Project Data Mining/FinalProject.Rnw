\documentclass[letterpaper]{article}
\input{helper/header} %Formatting, macros, package list, etc

\title{Final Project Thorough Report}
%\subtitle{}
\assignedDate{2016-09-30}
\dueDate{2016-12-10}
\studentName{Douglas Rowe}


\begin{document}
\SweaveOpts{concordance=TRUE}
<<echo=FALSE, eval=TRUE, include=FALSE>>=
library(knitr)
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, include=TRUE, tidy=TRUE, results='tex', fig.width=4, fig.asp=1, fig.align='center')
render_listings()
library(e1071)
#source("C:/Users/Rowe/Documents/My Repo/my_functions.R")
library(class)
library(survival)
library(Formula)
library(splancs)
library(sp)
library(kknn)
library(Hmisc)
library(shapefiles)
library(foreign)
library(aspace)
library(rpart)
library(rpart.plot)
library(rattle)
library(adabag)
library(mlbench)
library(caret)
library(lattice)
library(ggplot2)
library(randomForest)
library(nnet)
trainsample=function(dataset,trainprop){
  return(sample(nrow(dataset),
                round(nrow(dataset)*trainprop,0)))
}
my_shapiro = function(dataset){
  normality = c(1:ncol(dataset))
  for(i in 1:ncol(dataset)){
    normality[i]= shapiro.test(dataset[,i])$p.value
  }
  return(normality)
}
@
\linenumbers
\paragraph{}This data set is called the abalone data set. Abalones are a form of sea snails whose age can be determined by boring into the shell and, through a "boring and time consuming" process, by counting the rings much like a tree. In order to save our conchologist friends' time, we will attempt to predict the number of rings in the shell by measuring:
\ben
  \item Sex
  \item Length
  \item Diameter
  \item Height
  \item Whole Weight
  \item Shucked Weight
  \item Viscera weight
  \item Shell weight
\een
Preprocessing:
<<>>=
data=read.csv("C:/Users/Rowe/My-Programs/abalone.data.txt", header = FALSE)
#We have enough data from the UCI website to know that none of the data points are missing values or Nan's.
table(data[,1])
#Because there is almost even number of "M", "F" and "I" values here, there is minimal chance of a class imbalance problem coming from this variable.
my_shapiro(data[,2:8])
#From this function, we can confidently say that none of the continuous variables are normal.
@
Beginning:
<<>>=
#This function calculates a 1x2 matrix whose values are the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE) respectively.
RMSE_MAE = function(true, predicted){
  error_matrix = matrix(nrow = 1, ncol = 2)
  error_matrix[1,1] = sqrt((1/length(true))*sum((predicted - true)^2))
  error_matrix[1,2]  = (1/length(true))*sum(abs(true - predicted))
  return(error_matrix)
}
#I am creating master train and test set so we can compare the acccuracies of the models directly
set.seed(5364)
split_data = splitdata(data, .7)
train = split_data$train
test = split_data$test
train_rows = split_data$train_rows
@
Artificial Neural Networks:
<<>>=
Error_matrix =  matrix(ncol = 2, nrow = 15)
for(i in 1:15){
  crude_ann = nnet(V9~., data = train, size = as.numeric(i), maxit = 1000, linout = TRUE, trace = FALSE)
  predicted_ann = predict(crude_ann, newdata = test)
  Error_matrix[i,] = RMSE_MAE(test[,9], predicted_ann)
}
Error_matrix[argmin(Error_matrix[,1]),]
Error_matrix[argmin(Error_matrix[,2]),]
RMSE_size = argmin(Error_matrix[,1])
MAE_size = argmin(Error_matrix[,2])
ann_cv = function(size, d, m, data_frame){
  error_matrix=matrix(nrow = m+1, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_train=data_frame[d_records,]
    temp_test=data_frame[-d_records,]
    model = nnet(V9~., data = temp_train, size = size, maxit = 1000, linout = TRUE, trace = FALSE)
    predicted = predict(model, temp_test)
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,9])
  }
  error_matrix[m+1,1] = mean(error_matrix[1:m,1])
  error_matrix[m+1,2] = mean(error_matrix[1:m,2])
  return(error_matrix)
}
ann_cv(RMSE_size, round(.7*nrow(data)), 25, data)
ann_cv(MAE_size, round(.7*nrow(data)), 25, data)
ann_n_fold_cross=function(n,data_frame,size){
  chopping_point=floor(nrow(data_frame)/n)
  stopping_points=c(1/chopping_point,1:n)*chopping_point
  error_matrix=matrix(ncol = 2, nrow = n+1)
  for (i in 1:n){
    lower_bound=stopping_points[i]
    upper_bound=stopping_points[i+1]
    temp_test=data_frame[lower_bound:upper_bound,]
    temp_train=data_frame[-(lower_bound:upper_bound),]
    model = nnet(V9~., data = temp_train, size = size, maxit = 1000, linout = TRUE, trace = FALSE)
    predicted = predict(model, temp_test)
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,9])
  }
  error_matrix[n+1,1]=mean(error_matrix[1:n,1])
  error_matrix[n+1,2]=mean(error_matrix[1:n,2])
  return(error_matrix)
}
ann_n_fold_cross(10, data, RMSE_size)
ann_n_fold_cross(10, data, MAE_size)
@
Decision Trees:
<<>>=
#First, we're going to create a decision tree to find a good guess as how accurate we can get it.
first_tree = rpart(V9~., data = train)
first_predicted = predict(first_tree, newdata = test)
RMSE_MAE(first_predicted, test[,9])
#Now that we have an estimate for accuracy, we're going to use randomForest to get a more accurate model using the same test and train data as the decision tree did. Note: like all tune functions, it does include a 10-fold cross validation test of accuracy.
forestmodel=tune.randomForest(V9~., data = train)
forestmodel
randomforest_cv = function(d, m, data_frame){
  error_matrix=matrix(nrow = m+1, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_train=data_frame[d_records,]
    temp_test=data_frame[-d_records,]
    model = randomForest(V9~., data = temp_train)
    predicted = predict(model, temp_test)
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,9])
  }
  error_matrix[m+1,1] = mean(error_matrix[1:m,1])
  error_matrix[m+1,2] = mean(error_matrix[1:m,2])
  return(error_matrix)
}
randomforest_cv(round(.7*nrow(data)), 25, data)
@
Support Vector Machines:
<<>>=
#Here I'm making a for loop that runs the tune function five times to find a good guess at the best parameters.
best_parameters = matrix(ncol = 2, nrow = 5)
obj<- tune.svm(V9~., data = train, gamma = 2^(0:4), cost = 2^(0:4))
best_parameters[1,] = as.matrix(obj$best.parameters)
for(i in 1:4){
  gamma = as.numeric(obj$best.parameters)[1]
  cost = as.numeric(obj$best.parameters)[2]
  obj<- tune.svm(V9~., data = train, gamma = (.25*gamma):(1.75*gamma), cost = (.5*cost):(1.5*cost))
  best_parameters[i+1,]= as.matrix(obj$best.parameters)
}
best_model = obj$best.model
svm_cv=function(d,m,data_frame){
  error_matrix=matrix(nrow = m+1, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_train=data_frame[d_records,]
    temp_test=data_frame[-d_records,]
    model = svm(V9~., data = temp_train, gamma = best_parameters[5,1], cost = best_parameters[5,2])
    predicted = as.numeric(predict(model, temp_test))
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,9])
  }
  error_matrix[m+1,1] = mean(error_matrix[1:m,1])
  error_matrix[m+1,2] = mean(error_matrix[1:m,2])
  return(error_matrix)
}
svm_cv(round(.7*nrow(data)), 25, data)
obj
@
Naive Bayes:
<<>>=
nrings = data[,9]
rings = as.factor(nrings)
sex = data[,1]
nb_data = data[,-1]
nb_train = train[,-1]
nb_test = test[,-1]
Error_matrix = matrix(nrow = 14, ncol = 2)
#Since the Shapiro-Wilks test suggests that all of the numeric variables are nonnormal, I have to find out the best way to turn them into discrete factors using a for loop. Thus, I am keeping the train and test set constant and only changing number of levels for the newly-factorized variables.
colnames(Error_matrix)= c("RMSE", "MAE")
for(i in 2:15){
  discretized_train = discretizer(nb_train[,-8], i)
  discretized_train = cbind(sex[train_rows], discretized_train)
  discretized_test = discretizer(nb_test[,-8],i)
  discretized_test = cbind(sex[-train_rows], discretized_test)
  model = naiveBayes(rings[train_rows]~., data = discretized_train)
  predicted_naive_Bayes = predict(model, test)
  predicted_naive_Bayes = as.numeric(predicted_naive_Bayes)
  Error_matrix[i-1,] = RMSE_MAE(predicted_naive_Bayes, test[,9])
}
Error_matrix[argmin(Error_matrix[,1]),]
Error_matrix[argmin(Error_matrix[,2]),]
#We can see that the factor levels that minimizes the Root Mean Square error and the Mean Absolute error are very close but different and both of them don't change all that much when switching between the two. So, we could confidently pick either one and would recieve similar results. I am now going to use 10-fold cross-validation to double check my numbers.
data_RMSE = discretizer(nb_data[,-8], argmin(Error_matrix[,1]))
data_RMSE = cbind(sex, data_RMSE)
data_MAE = discretizer(nb_data[,-8], argmin(Error_matrix[,2]))
data_MAE = cbind(sex, data_MAE)
data = cbind(sex, data)
naiveBayes_cv=function(d,m,data_frame){
  error_matrix=matrix(nrow = m+1, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_test=data_frame[d_records,]
    temp_train=data_frame[-d_records,]
    model = naiveBayes(rings[-d_records]~., data = temp_train)
    predicted = as.numeric(predict(model, temp_test))
    error_matrix[i,] = RMSE_MAE(predicted, nrings[d_records])
  }
  error_matrix[m+1,1] = mean(error_matrix[1:m,1])
  error_matrix[m+1,2] = mean(error_matrix[1:m,2])
  return(error_matrix)
}
naiveBayes_cv(round(.7*nrow(data_RMSE)), 25, data_RMSE)
naiveBayes_cv(round(.7*nrow(data_MAE)), 25, data_MAE)
nb_n_fold_cross=function(n,data_frame){
  chopping_point=floor(nrow(data_frame)/n)
  stopping_points=c(1/chopping_point,1:n)*chopping_point
  error_matrix=matrix(ncol = 2, nrow = n+1)
  for (i in 1:n){
    lower_bound=stopping_points[i]
    upper_bound=stopping_points[i+1]
    temp_test=data_frame[lower_bound:upper_bound,]
    temp_train=data_frame[-(lower_bound:upper_bound),]
    model = naiveBayes(rings[-(lower_bound:upper_bound)]~., data = temp_train)
    predicted = as.numeric(predict(model, temp_test))
    error_matrix[i,] = RMSE_MAE(predicted, nrings[lower_bound:upper_bound])
  }
  error_matrix[n+1,1]=mean(error_matrix[1:n,1])
  error_matrix[n+1,2]=mean(error_matrix[1:n,2])
  return(error_matrix)
}
nb_n_fold_cross(10, data_RMSE)
nb_n_fold_cross(10, data_MAE)
@
Knn function:
<<>>=
#Because knn can't calculate distance with factor variables, we have to create two dummy variables. Also, because knn relies on distance, I scaled the data so that variables who have smaller ranges aren't more important.
male = as.numeric(data[,1]=="M")
female = as.numeric(data[,1]=="F")
knn_data = cbind(male, female, data[,-1])
knn_train = cbind(male[train_rows], female[train_rows], train[,-1])
knn_test = cbind(male[-train_rows], female[-train_rows], test[,-1])
Error_matrix = matrix(ncol = 2, nrow = 50)
for(i in 1:50){
  predicted_knn = as.numeric(knn(knn_train[,-10], knn_test[,-10], knn_train[,10], k = i))
  Error_matrix[i,] = cbind(RMSE_MAE(knn_test[,10], predicted_knn))
}
Error_matrix[argmin(Error_matrix[,1]),]
Error_matrix[argmin(Error_matrix[,2]),]
k_RMSE = argmin(Error_matrix[,1])
k_MAE = argmin(Error_matrix[,2])
#Delete-d cross validation
knn_cv=function(d,m,data_frame, k){
  error_matrix=matrix(nrow = m+1, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_test=data_frame[-d_records,]
    temp_train=data_frame[d_records,]
    predicted_knn = as.numeric(knn(temp_train[,-10], temp_test[,-10], cl = temp_train[,10], k = k))
    error_matrix[i,] = RMSE_MAE(temp_test[,10], predicted_knn)
  }
  error_matrix[m+1,1] = mean(error_matrix[1:m,1])
  error_matrix[m+1,2] = mean(error_matrix[1:m,2])
  return(error_matrix)
}
knn_cv(round(.7*nrow(knn_data)), 25, knn_data, k = k_RMSE)
knn_cv(round(.7*nrow(knn_data)), 25, knn_data, k = k_MAE)
#10-fold cross validation
knn_n_fold_cross=function(n,data_frame, k){
  chopping_point=floor(nrow(data_frame)/n)
  stopping_points=c(1/chopping_point,1:n)*chopping_point
  error_matrix=matrix(ncol = 2, nrow = n+1)
  for (i in 1:n){
    lower_bound=stopping_points[i]
    upper_bound=stopping_points[i+1]
    temp_test=data_frame[lower_bound:upper_bound,]
    temp_train=data_frame[-(lower_bound:upper_bound),]
    predicted = as.numeric(knn(temp_train[,-10], temp_test[,-10], cl = temp_train[,10], k = k))
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,10])
  }
  error_matrix[n+1,1]=mean(error_matrix[1:n,1])
  error_matrix[n+1,2]=mean(error_matrix[1:n,2])
  return(error_matrix)
}
knn_n_fold_cross(10, knn_data, k_RMSE)
knn_n_fold_cross(10, knn_data, k_MAE)
@
Weighted Knn-function:
<<>>=
k_vector=c(1:50)
#Since the optimal kernel does not always provide the best result, all of them are being tested with k values between 1 and 50. Once the optimal kernal and k value are calculated, they will cross validated.
  optimal_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel="optimal",ykernel=-1.2:1.2)
  triangular_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel="triangular",ykernel=-1.2:1.2 )
  gaussian_test=train.kknn(V9~.,data= knn_data, ks=k_vector, kernel="gaussian",ykernel=-1.2:1.2 )
  epanechnikov_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel="epanechnikov",ykernel=-1.2:1.2 )
  biweight_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel="biweight",ykernel=-1.2:1.2 )
  triweight_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel="triweight",ykernel=-1.2:1.2 )
  cosine_test=train.kknn(V9~.,data= knn_data, ks=k_vector, kernel="cos",ykernel=-1.2:1.2 )
  inverted_test=train.kknn(V9~.,data= knn_data, ks=k_vector, kernel="inv",ykernel=-1.2:1.2 )
  rectangular_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel = "rectangular",ykernel = -1.2:1.2)
  optimal_data_frame=rbind(c("optimal",min(optimal_test$MEAN.ABS), min(optimal_test$MEAN.SQU), optimal_test$best.parameters$k), c("triangular", min(triangular_test$MEAN.ABS), min(triangular_test$MEAN.SQU),triangular_test$best.parameters$k),c("epanechnikov",min(epanechnikov_test$MEAN.ABS),min(epanechnikov_test$MEAN.SQU),epanechnikov_test$best.parameters$k),c("biweight",min(biweight_test$MEAN.ABS), min(biweight_test$MEAN.SQU),biweight_test$best.parameters$k),c("triweight", min(triweight_test$MEAN.ABS), min(triweight_test$MEAN.SQU), triweight_test$best.parameters$k),c("cos",min(cosine_test$MEAN.ABS), min(cosine_test$MEAN.SQU),cosine_test$best.parameters$k),c("inv",min(inverted_test$MEAN.ABS), min(inverted_test$MEAN.SQU),inverted_test$best.parameters$k),c("gaussian",min(gaussian_test$MEAN.ABS),min(gaussian_test$MEAN.SQU),gaussian_test$best.parameters$k),c("rectangular",min(rectangular_test$MEAN.ABS),min(rectangular_test$MEAN.SQU),rectangular_test$best.parameters$k))
  #Since train.kknn includes a leave-one-out cross validation technique, I will use that method.
  optimal_data_frame[argmin(optimal_data_frame[,2]),]
  optimal_data_frame[argmin(optimal_data_frame[,3]),]
  optimal_k_abs = optimal_data_frame[argmin(optimal_data_frame[,2]),4]
  optimal_k_abs = as.numeric(optimal_k_abs)
  optimal_kernel_abs = optimal_data_frame[argmin(optimal_data_frame[,2]),1]
  optimal_kernel_abs = as.character(optimal_kernel_abs)
  optimal_k_rmse = optimal_data_frame[argmin(optimal_data_frame[,3]),4]
  optimal_k_rmse = as.numeric(optimal_k_rmse)
  optimal_kernel_rmse = optimal_data_frame[argmin(optimal_data_frame[,3]),1]
  optimal_kernel_rmse = as.character(optimal_kernel_rmse)
#10 fold cross validation using convenience command
fold_RMSE = cv.kknn(V9~., data = knn_data, kcv = 10, k = optimal_k_rmse, kernel = optimal_kernel_rmse)
RMSE_MAE(fold_RMSE[[1]][1:4177],fold_RMSE[[1]][4178:(2*4177)])
fold_MAE = cv.kknn(V9~., data = knn_data, kcv = 10, k = optimal_k_abs, kernel = optimal_kernel_abs)
RMSE_MAE(fold_MAE[[1]][1:4177],fold_MAE[[1]][4178:(2*4177)])
@

\end{document}