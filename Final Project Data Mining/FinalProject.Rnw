\documentclass[letterpaper]{article}
\input{helper/header} %Formatting, macros, package list, etc

\title{Final Project Thorough Report}
%\subtitle{}
\assignedDate{2016-09-30}
\dueDate{2016-12-10}
\studentName{Douglas Rowe}


\begin{document}
\SweaveOpts{concordance=TRUE}
<<echo=FALSE, eval=TRUE, include=FALSE>>=
library(knitr)
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, include=TRUE, tidy=TRUE, results='tex', fig.width=4, fig.asp=1, fig.align='center')
render_listings()
library(e1071)
library(class)
library(survival)
library(Formula)
#library(splancs)
library(sp)
source('~/R/My Packages/kknn/R/kknn.R')
source('~/R/My Packages/kknn/R/specClust.R')
#library(Hmisc)
library(shapefiles)
library(foreign)
#library(aspace)
library(rpart)
library(rpart.plot)
#library(rattle)
#library(adabag)
library(mlbench)
#library(caret)
library(lattice)
library(ggplot2)
#library(randomForest)
library(nnet)
trainsample=function(dataset,trainprop){
  return(sample(nrow(dataset),
                round(nrow(dataset)*trainprop,0)))
}
my_shapiro = function(dataset){
  normality = c(1:ncol(dataset))
  for(i in 1:ncol(dataset)){
    normality[i]= shapiro.test(dataset[,i])$p.value
  }
  return(normality)
}
splitdata=function(data_frame,training_fraction){
  size_train<-ceiling(nrow(data_frame)*training_fraction)
  train_rows<-sample(nrow(data_frame),size_train)
  training<-data_frame[train_rows,]
  test<-data_frame[-train_rows,]
  #splitlist<-list(training,test)
  list(train=training, test=test,train_rows=train_rows)
}
argmin = function(vector){return(which.min(vector))}
argmax = function(vector){return(which.max(vector))}

@
\linenumbers
\paragraph{}This data set is called the abalone data set. Abalones are a form of sea snails whose age can be determined by boring into the shell and, through a "boring and time consuming" process, by counting the rings much like a tree. In order to save our conchologist friends' time, we will attempt to predict the number of rings in the shell by measuring:
\ben
  \item Sex
  \item Length
  \item Diameter
  \item Height
  \item Whole Weight
  \item Shucked Weight
  \item Viscera weight
  \item Shell weight
\een
Preprocessing:
\indent We already know from the UCI website that none of the data points are missing values or Nan's. Now all we have to see is if there is a class imbalance problem. As you can tell from below, there will not be class-imbalance. Also, by the Shapiro-Wilks test, we can see that none of the columns are normally distributed.
<<>>=
data=read.csv("~/My-Programs/Final Project Data Mining/abalone.data.txt", header = FALSE)
table(data[,1])
my_shapiro(data[,2:8])
@

\indent Before we start running some real code, we're going to create a function that return the Root Mean Square Error (RMSE) and the Mean Absolute Error (MAE) and we're going to create test and training sets for the algorithms. For each one, we're going to tune the algorithm as best we can and then cross validate the scores for each one using delete-d and some form of k-fold cross validation. Then, we're going to use the cross-validation scores to compare algorithms.
<<>>=
RMSE_MAE = function(true, predicted){
  error_matrix = matrix(nrow = 1, ncol = 2)
  error_matrix[1,1] = sqrt((1/length(true))*sum((predicted - true)^2))
  error_matrix[1,2]  = (1/length(true))*sum(abs(true - predicted))
  return(error_matrix)
}
set.seed(5364)
split_data = splitdata(data, .7)
train = split_data$train
test = split_data$test
train_rows = split_data$train_rows
@
Artificial Neural Networks:
<<>>=
Error_matrix =  matrix(ncol = 2, nrow = 15)
for(i in 1:15){
  crude_ann = nnet(V9~., data = train, size = as.numeric(i), maxit = 1000, linout = TRUE, trace = FALSE)
  predicted_ann = predict(crude_ann, newdata = test)
  Error_matrix[i,] = RMSE_MAE(test[,9], predicted_ann)
}
Error_matrix[argmin(Error_matrix[,2]),]
best_size = argmin(Error_matrix[,2])
ann_cv = function(size, d, m, data_frame){
  error_matrix=matrix(nrow = m, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_train=data_frame[d_records,]
    temp_test=data_frame[-d_records,]
    model = nnet(V9~., data = temp_train, size = size, maxit = 1000, linout = TRUE, trace = FALSE)
    predicted = predict(model, temp_test)
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,9])
  }
  return(list("RMSE average" = mean(error_matrix[,1]), "MAE average" =mean(error_matrix[,2])))
}
ann_n_fold_cross=function(n,data_frame,size){
  chopping_point=floor(nrow(data_frame)/n)
  stopping_points=c(1/chopping_point,1:n)*chopping_point
  error_matrix=matrix(ncol = 2, nrow = n)
  for (i in 1:n){
    lower_bound=stopping_points[i]
    upper_bound=stopping_points[i+1]
    temp_test=data_frame[lower_bound:upper_bound,]
    temp_train=data_frame[-(lower_bound:upper_bound),]
    model = nnet(V9~., data = temp_train, size = size, maxit = 1000, linout = TRUE, trace = FALSE)
    predicted = predict(model, temp_test)
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,9])
  }
  return(list("RMSE average" = mean(error_matrix[,1]),"MAE average" = mean(error_matrix[,2])))
}
ann_cv(best_size, round(nrow(data)*.7), 25, data)
ann_n_fold_cross(10, data, best_size)
@
Decision Trees:
<<>>=
#First, we're going to create a decision tree to find a good guess as how accurate we can get it.
first_tree = rpart(V9~., data = train)
first_predicted = predict(first_tree, newdata = test)
RMSE_MAE(first_predicted, test[,9])
#Now that we have an estimate for accuracy, we're going to use randomForest to get a more accurate model using the same test and train data as the decision tree did. Note: like all tune functions, it does include a 10-fold cross validation test of accuracy.
forestmodel=randomForest(V9~., data = train)
forestmodel
randomforest_cv = function(d, m, data_frame){
  error_matrix=matrix(nrow = m+1, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_train=data_frame[d_records,]
    temp_test=data_frame[-d_records,]
    model = randomForest(V9~., data = temp_train)
    predicted = predict(model, temp_test)
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,9])
  }
  error_matrix[m+1,1] = mean(error_matrix[1:m,1])
  error_matrix[m+1,2] = mean(error_matrix[1:m,2])
  return(error_matrix)
}
randomforest_cv(round(.7*nrow(data)), 25, data)
@
Support Vector Machines:
<<>>=
#Here I'm making a for loop that runs the tune function five times to find a good guess at the best parameters.
best_parameters = matrix(ncol = 2, nrow = 5)
obj<- tune.svm(V9~., data = train, gamma = 2^(0:4), cost = 2^(0:4))
best_parameters[1,] = as.matrix(obj$best.parameters)
for(i in 1:4){
  gamma = as.numeric(obj$best.parameters)[1]
  cost = as.numeric(obj$best.parameters)[2]
  obj<- tune.svm(V9~., data = train, gamma = (.25*gamma):(1.75*gamma), cost = (.5*cost):(1.5*cost))
  best_parameters[i+1,]= as.matrix(obj$best.parameters)
}
best_model = obj$best.model
svm_cv=function(d,m,data_frame){
  error_matrix=matrix(nrow = m+1, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_train=data_frame[d_records,]
    temp_test=data_frame[-d_records,]
    model = svm(V9~., data = temp_train, gamma = best_parameters[5,1], cost = best_parameters[5,2])
    predicted = as.numeric(predict(model, temp_test))
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,9])
  }
  error_matrix[m+1,1] = mean(error_matrix[1:m,1])
  error_matrix[m+1,2] = mean(error_matrix[1:m,2])
  return(error_matrix)
}
svm_cv(round(.7*nrow(data)), 25, data)
obj
@
Naive Bayes:
<<>>=
nrings = data[,9]
rings = as.factor(nrings)
sex = data[,1]
nb_data = data[,-1]
nb_train = train[,-1]
nb_test = test[,-1]
Error_matrix = matrix(nrow = 14, ncol = 2)
#Since the Shapiro-Wilks test suggests that all of the numeric variables are nonnormal, I have to find out the best way to turn them into discrete factors using a for loop. Thus, I am keeping the train and test set constant and only changing number of levels for the newly-factorized variables.
colnames(Error_matrix)= c("RMSE", "MAE")
for(i in 2:15){
  discretized_train = discretizer(nb_train[,-8], i)
  discretized_train = cbind(sex[train_rows], discretized_train)
  discretized_test = discretizer(nb_test[,-8],i)
  discretized_test = cbind(sex[-train_rows], discretized_test)
  model = naiveBayes(rings[train_rows]~., data = discretized_train)
  predicted_naive_Bayes = predict(model, test)
  predicted_naive_Bayes = as.numeric(predicted_naive_Bayes)
  Error_matrix[i-1,] = RMSE_MAE(predicted_naive_Bayes, test[,9])
}
Error_matrix[argmin(Error_matrix[,1]),]
Error_matrix[argmin(Error_matrix[,2]),]
#We can see that the factor levels that minimizes the Root Mean Square error and the Mean Absolute error are very close but different and both of them don't change all that much when switching between the two. So, we could confidently pick either one and would recieve similar results. I am now going to use 10-fold cross-validation to double check my numbers.
data_RMSE = discretizer(nb_data[,-8], argmin(Error_matrix[,1]))
data_RMSE = cbind(sex, data_RMSE)
data_MAE = discretizer(nb_data[,-8], argmin(Error_matrix[,2]))
data_MAE = cbind(sex, data_MAE)
data = cbind(sex, data)
naiveBayes_cv=function(d,m,data_frame){
  error_matrix=matrix(nrow = m+1, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_test=data_frame[d_records,]
    temp_train=data_frame[-d_records,]
    model = naiveBayes(rings[-d_records]~., data = temp_train)
    predicted = as.numeric(predict(model, temp_test))
    error_matrix[i,] = RMSE_MAE(predicted, nrings[d_records])
  }
  error_matrix[m+1,1] = mean(error_matrix[1:m,1])
  error_matrix[m+1,2] = mean(error_matrix[1:m,2])
  return(error_matrix)
}
naiveBayes_cv(round(.7*nrow(data_RMSE)), 25, data_RMSE)
naiveBayes_cv(round(.7*nrow(data_MAE)), 25, data_MAE)
nb_n_fold_cross=function(n,data_frame){
  chopping_point=floor(nrow(data_frame)/n)
  stopping_points=c(1/chopping_point,1:n)*chopping_point
  error_matrix=matrix(ncol = 2, nrow = n+1)
  for (i in 1:n){
    lower_bound=stopping_points[i]
    upper_bound=stopping_points[i+1]
    temp_test=data_frame[lower_bound:upper_bound,]
    temp_train=data_frame[-(lower_bound:upper_bound),]
    model = naiveBayes(rings[-(lower_bound:upper_bound)]~., data = temp_train)
    predicted = as.numeric(predict(model, temp_test))
    error_matrix[i,] = RMSE_MAE(predicted, nrings[lower_bound:upper_bound])
  }
  error_matrix[n+1,1]=mean(error_matrix[1:n,1])
  error_matrix[n+1,2]=mean(error_matrix[1:n,2])
  return(error_matrix)
}
nb_n_fold_cross(10, data_RMSE)
nb_n_fold_cross(10, data_MAE)
@
Knn function:
<<>>=
#Because knn can't calculate distance with factor variables, we have to create two dummy variables. Also, because knn relies on distance, I scaled the data so that variables who have smaller ranges aren't more important.
male = as.numeric(data[,1]=="M")
female = as.numeric(data[,1]=="F")
knn_data = cbind(male, female, data[,-1])
knn_train = cbind(male[train_rows], female[train_rows], train[,-1])
knn_test = cbind(male[-train_rows], female[-train_rows], test[,-1])
Error_matrix = matrix(ncol = 2, nrow = 50)
for(i in 1:50){
  predicted_knn = as.numeric(knn(knn_train[,-10], knn_test[,-10], knn_train[,10], k = i))
  Error_matrix[i,] = cbind(RMSE_MAE(knn_test[,10], predicted_knn))
}
Error_matrix[argmin(Error_matrix[,1]),]
Error_matrix[argmin(Error_matrix[,2]),]
k_RMSE = argmin(Error_matrix[,1])
k_MAE = argmin(Error_matrix[,2])
#Delete-d cross validation
knn_cv=function(d,m,data_frame, k){
  error_matrix=matrix(nrow = m+1, ncol = 2)
  for (i in 1:m){
    d_records=sample(nrow(data_frame),d)
    temp_test=data_frame[-d_records,]
    temp_train=data_frame[d_records,]
    predicted_knn = as.numeric(knn(temp_train[,-10], temp_test[,-10], cl = temp_train[,10], k = k))
    error_matrix[i,] = RMSE_MAE(temp_test[,10], predicted_knn)
  }
  error_matrix[m+1,1] = mean(error_matrix[1:m,1])
  error_matrix[m+1,2] = mean(error_matrix[1:m,2])
  return(error_matrix)
}
knn_cv(round(.7*nrow(knn_data)), 25, knn_data, k = k_RMSE)
knn_cv(round(.7*nrow(knn_data)), 25, knn_data, k = k_MAE)
#10-fold cross validation
knn_n_fold_cross=function(n,data_frame, k){
  chopping_point=floor(nrow(data_frame)/n)
  stopping_points=c(1/chopping_point,1:n)*chopping_point
  error_matrix=matrix(ncol = 2, nrow = n+1)
  for (i in 1:n){
    lower_bound=stopping_points[i]
    upper_bound=stopping_points[i+1]
    temp_test=data_frame[lower_bound:upper_bound,]
    temp_train=data_frame[-(lower_bound:upper_bound),]
    predicted = as.numeric(knn(temp_train[,-10], temp_test[,-10], cl = temp_train[,10], k = k))
    error_matrix[i,] = RMSE_MAE(predicted, temp_test[,10])
  }
  error_matrix[n+1,1]=mean(error_matrix[1:n,1])
  error_matrix[n+1,2]=mean(error_matrix[1:n,2])
  return(error_matrix)
}
knn_n_fold_cross(10, knn_data, k_RMSE)
knn_n_fold_cross(10, knn_data, k_MAE)
@
Weighted Knn-function:
<<>>=
k_vector=c(1:50)
#Since the optimal kernel does not always provide the best result, all of them are being tested with k values between 1 and 50. Once the optimal kernal and k value are calculated, they will cross validated.
  optimal_test=train.kknn(V9~.,data = knn_data, distance = 1,ks=k_vector, kernel="optimal",ykernel=-1.2:1.2)
  triangular_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel="triangular",ykernel=-1.2:1.2 )
  gaussian_test=train.kknn(V9~.,data= knn_data, ks=k_vector, kernel="gaussian",ykernel=-1.2:1.2 )
  epanechnikov_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel="epanechnikov",ykernel=-1.2:1.2 )
  biweight_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel="biweight",ykernel=-1.2:1.2 )
  triweight_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel="triweight",ykernel=-1.2:1.2 )
  cosine_test=train.kknn(V9~.,data= knn_data, ks=k_vector, kernel="cos",ykernel=-1.2:1.2 )
  inverted_test=train.kknn(V9~.,data= knn_data, ks=k_vector, kernel="inv",ykernel=-1.2:1.2 )
  rectangular_test=train.kknn(V9~.,data = knn_data, ks=k_vector, kernel = "rectangular",ykernel = -1.2:1.2)
  optimal_data_frame=rbind(c("optimal",min(optimal_test$MEAN.ABS), min(optimal_test$MEAN.SQU), optimal_test$best.parameters$k), c("triangular", min(triangular_test$MEAN.ABS), min(triangular_test$MEAN.SQU),triangular_test$best.parameters$k),c("epanechnikov",min(epanechnikov_test$MEAN.ABS),min(epanechnikov_test$MEAN.SQU),epanechnikov_test$best.parameters$k),c("biweight",min(biweight_test$MEAN.ABS), min(biweight_test$MEAN.SQU),biweight_test$best.parameters$k),c("triweight", min(triweight_test$MEAN.ABS), min(triweight_test$MEAN.SQU), triweight_test$best.parameters$k),c("cos",min(cosine_test$MEAN.ABS), min(cosine_test$MEAN.SQU),cosine_test$best.parameters$k),c("inv",min(inverted_test$MEAN.ABS), min(inverted_test$MEAN.SQU),inverted_test$best.parameters$k),c("gaussian",min(gaussian_test$MEAN.ABS),min(gaussian_test$MEAN.SQU),gaussian_test$best.parameters$k),c("rectangular",min(rectangular_test$MEAN.ABS),min(rectangular_test$MEAN.SQU),rectangular_test$best.parameters$k))
  #Since train.kknn includes a leave-one-out cross validation technique, I will use that method.
  optimal_data_frame[argmin(optimal_data_frame[,2]),]
  optimal_data_frame[argmin(optimal_data_frame[,3]),]
  optimal_k_abs = optimal_data_frame[argmin(optimal_data_frame[,2]),4]
  optimal_k_abs = as.numeric(optimal_k_abs)
  optimal_kernel_abs = optimal_data_frame[argmin(optimal_data_frame[,2]),1]
  optimal_kernel_abs = as.character(optimal_kernel_abs)
  optimal_k_rmse = optimal_data_frame[argmin(optimal_data_frame[,3]),4]
  optimal_k_rmse = as.numeric(optimal_k_rmse)
  optimal_kernel_rmse = optimal_data_frame[argmin(optimal_data_frame[,3]),1]
  optimal_kernel_rmse = as.character(optimal_kernel_rmse)
#10 fold cross validation using convenience command
fold_RMSE = cv.kknn(V9~., data = knn_data, kcv = 10, k = optimal_k_rmse, kernel = optimal_kernel_rmse)
RMSE_MAE(fold_RMSE[[1]][1:4177],fold_RMSE[[1]][4178:(2*4177)])
fold_MAE = cv.kknn(V9~., data = knn_data, kcv = 10, k = optimal_k_abs, kernel = optimal_kernel_abs)
RMSE_MAE(fold_MAE[[1]][1:4177],fold_MAE[[1]][4178:(2*4177)])
@

\end{document}